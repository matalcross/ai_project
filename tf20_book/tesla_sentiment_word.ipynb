{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e38fd48b",
   "metadata": {},
   "source": [
    "# 자연어 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "822a6608",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2272ceb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_file = tf.keras.utils.get_file('input.txt', 'http://bit.ly/2Mc3SOV')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b0f286",
   "metadata": {},
   "source": [
    "# 데이터 로드 및 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b0e3d1d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of text: 26265493 characters\n",
      "\n",
      "﻿태조 이성계 선대의 가계. 목조 이안사가 전주에서 삼척·의주를 거쳐 알동에 정착하다 \n",
      "태조 강헌 지인 계운 성문 신무 대왕(太祖康獻至仁啓運聖文神武大王)의 성은 이씨(李氏)요, 휘\n"
     ]
    }
   ],
   "source": [
    "# 7.33 데이터 로드 및 확인\n",
    "# 데이터를 메모리에 불러옵니다. encoding 형식으로 utf-8 을 지정해야합니다.\n",
    "train_text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
    "\n",
    "# 텍스트가 총 몇 자인지 확인합니다.\n",
    "print('Length of text: {} characters'.format(len(train_text)))\n",
    "print()\n",
    "\n",
    "# 처음 100 자를 확인해봅니다.\n",
    "print(train_text[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b5e0e3c",
   "metadata": {},
   "source": [
    "# 훈련 데이터 입력 정제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de8bb552",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.34 훈련 데이터 입력 정제\n",
    "import re\n",
    "# From https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py\n",
    "def clean_str(string):    \n",
    "    string = re.sub(r\"[^가-힣A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
    "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
    "    string = re.sub(r\",\", \" , \", string)\n",
    "    string = re.sub(r\"!\", \" ! \", string)\n",
    "    string = re.sub(r\"\\(\", \"\", string)\n",
    "    string = re.sub(r\"\\)\", \"\", string)\n",
    "    string = re.sub(r\"\\?\", \" \\? \", string)\n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "    string = re.sub(r\"\\'{2,}\", \"\\'\", string)\n",
    "    string = re.sub(r\"\\'\", \"\", string)\n",
    "\n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8262f87e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['태조', '이성계', '선대의', '가계', '목조', '이안사가', '전주에서', '삼척', '의주를', '거쳐', '알동에', '정착하다', '\\n', '태조', '강헌', '지인', '계운', '성문', '신무', '대왕']\n"
     ]
    }
   ],
   "source": [
    "train_text = train_text.split('\\n')\n",
    "train_text = [clean_str(sentence) for sentence in train_text]\n",
    "train_text_X = []\n",
    "for sentence in train_text:\n",
    "    train_text_X.extend(sentence.split(' '))\n",
    "    train_text_X.append('\\n')\n",
    "    \n",
    "train_text_X = [word for word in train_text_X if word != '']\n",
    "\n",
    "print(train_text_X[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd2496df",
   "metadata": {},
   "source": [
    "# 단어 토크화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "953361a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "332640 unique words\n",
      "{\n",
      "  '\\n':   0,\n",
      "  '!' :   1,\n",
      "  ',' :   2,\n",
      "  '000명으로':   3,\n",
      "  '001':   4,\n",
      "  '002':   5,\n",
      "  '003':   6,\n",
      "  '004':   7,\n",
      "  '005':   8,\n",
      "  '006':   9,\n",
      "  ...\n",
      "}\n",
      "index of UNK: 332639\n"
     ]
    }
   ],
   "source": [
    "# 7.35 단어 토큰화\n",
    "# 단어의 set을 만듭니다.\n",
    "vocab = sorted(set(train_text_X))\n",
    "vocab.append('UNK')\n",
    "print ('{} unique words'.format(len(vocab)))\n",
    "\n",
    "# vocab list를 숫자로 맵핑하고, 반대도 실행합니다.\n",
    "word2idx = {u:i for i, u in enumerate(vocab)}\n",
    "idx2word = np.array(vocab)\n",
    "\n",
    "text_as_int = np.array([word2idx[c] for c in train_text_X])\n",
    "\n",
    "# word2idx 의 일부를 알아보기 쉽게 print 해봅니다.\n",
    "print('{')\n",
    "for word,_ in zip(word2idx, range(10)):\n",
    "    print('  {:4s}: {:3d},'.format(repr(word), word2idx[word]))\n",
    "print('  ...\\n}')\n",
    "\n",
    "print('index of UNK: {}'.format(word2idx['UNK']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe7e30a",
   "metadata": {},
   "source": [
    "# 토큰 데이터 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bf34e483",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['태조', '이성계', '선대의', '가계', '목조', '이안사가', '전주에서', '삼척', '의주를', '거쳐', '알동에', '정착하다', '\\n', '태조', '강헌', '지인', '계운', '성문', '신무', '대왕']\n",
      "[299305 229634 161443  17430 111029 230292 251081 155087 225462  29027\n",
      " 190295 256129      0 299305  25624 273553  36147 163996 180466  84413]\n"
     ]
    }
   ],
   "source": [
    "# 7.36 토큰 데이터 확인\n",
    "print(train_text_X[:20])\n",
    "print(text_as_int[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed1e4c49",
   "metadata": {},
   "source": [
    "# 기본 데이터세트 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c7c329dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1\n",
      "['태조' '이성계' '선대의' '가계' '목조' '이안사가' '전주에서' '삼척' '의주를' '거쳐' '알동에' '정착하다'\n",
      " '\\n' '태조' '강헌' '지인' '계운' '성문' '신무' '대왕' '의' '성은' '이씨' '요' ',' '휘']\n",
      "[299305 229634 161443  17430 111029 230292 251081 155087 225462  29027\n",
      " 190295 256129      0 299305  25624 273553  36147 163996 180466  84413\n",
      " 224182 164549 230248 210912      2 330313]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-23 14:27:13.729027: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2022-02-23 14:27:13.730123: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "# 7.37 기본 데이터셋 만들기\n",
    "seq_length = 25\n",
    "examples_per_epoch = len(text_as_int) // seq_length\n",
    "sentence_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
    "\n",
    "sentence_dataset = sentence_dataset.batch(seq_length+1, drop_remainder=True)\n",
    "for item in sentence_dataset.take(1):\n",
    "    print(idx2word[item.numpy()])\n",
    "    print(item.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87cd19eb",
   "metadata": {},
   "source": [
    "# 학습 데이터세트 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4c574264",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['태조' '이성계' '선대의' '가계' '목조' '이안사가' '전주에서' '삼척' '의주를' '거쳐' '알동에' '정착하다'\n",
      " '\\n' '태조' '강헌' '지인' '계운' '성문' '신무' '대왕' '의' '성은' '이씨' '요' ',']\n",
      "[299305 229634 161443  17430 111029 230292 251081 155087 225462  29027\n",
      " 190295 256129      0 299305  25624 273553  36147 163996 180466  84413\n",
      " 224182 164549 230248 210912      2]\n",
      "휘\n",
      "330313\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-23 14:27:18.952692: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    }
   ],
   "source": [
    "# 7.38 학습 데이터셋 만들기\n",
    "def split_input_target(chunk):\n",
    "    return [chunk[:-1], chunk[-1]]\n",
    "\n",
    "train_dataset = sentence_dataset.map(split_input_target)\n",
    "for x,y in train_dataset.take(1):\n",
    "    print(idx2word[x.numpy()])\n",
    "    print(x.numpy())\n",
    "    print(idx2word[y.numpy()])\n",
    "    print(y.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a75dc4c",
   "metadata": {},
   "source": [
    "# 데이터세트 shuffle, batch 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e95b9299",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.39 데이터셋 shuffle, batch 설정\n",
    "BATCH_SIZE = 512\n",
    "steps_per_epoch = examples_per_epoch // BATCH_SIZE\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "train_dataset = train_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a68178b1",
   "metadata": {},
   "source": [
    "# 단어 단위 생성 모델 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "17f4a3dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 25, 100)           33264000  \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 25, 100)           80400     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 25, 100)           0         \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 100)               80400     \n",
      "                                                                 \n",
      " dense (Dense)               (None, 332640)            33596640  \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 67,021,440\n",
      "Trainable params: 67,021,440\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 7.40 단어 단위 생성 모델 정의\n",
    "total_words = len(vocab)\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(total_words, 100, input_length=seq_length),\n",
    "    tf.keras.layers.LSTM(units=100, return_sequences=True),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.LSTM(units=100),\n",
    "    tf.keras.layers.Dense(total_words, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd186ac8",
   "metadata": {},
   "source": [
    "# 단어 단위 생성 모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cab14e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-23 14:27:32.171681: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-23 14:27:32.679388: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-23 14:27:33.114856: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-23 14:27:33.387563: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-23 14:27:34.691662: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-23 14:36:20.052176: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-23 14:36:20.391401: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-02-23 14:36:21.057762: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 태조 이성계 선대의 가계 목조 이안사가 전주에서 삼척 의주를 거쳐 알동에 정착하다  , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , ,\n",
      "\n",
      "533/533 - 533s - loss: 9.4307 - accuracy: 0.0718 - 533s/epoch - 1000ms/step\n",
      "Epoch 2/50\n",
      "533/533 - 532s - loss: 8.6104 - accuracy: 0.0727 - 532s/epoch - 997ms/step\n",
      "Epoch 3/50\n",
      "533/533 - 528s - loss: 8.5094 - accuracy: 0.0722 - 528s/epoch - 991ms/step\n",
      "Epoch 4/50\n",
      "533/533 - 539s - loss: 8.4320 - accuracy: 0.0721 - 539s/epoch - 1s/step\n",
      "Epoch 5/50\n",
      "533/533 - 531s - loss: 8.3478 - accuracy: 0.0726 - 531s/epoch - 995ms/step\n",
      "Epoch 6/50\n"
     ]
    }
   ],
   "source": [
    "# tensorflow 버전 2.6이후로 predict_classes가 없기 때문에 발생하는 오류라고 한다. \n",
    "# predict_classes 대신 다음 코드로 대체하면 된다.\n",
    "# 7.41 단어 단위 생성 모델 학습\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "def testmodel(epoch, logs):\n",
    "    if epoch % 5 != 0 and epoch != 49:\n",
    "        return\n",
    "    test_sentence = train_text[0]\n",
    "\n",
    "    next_words = 100\n",
    "    for _ in range(next_words):\n",
    "        test_text_X = test_sentence.split(' ')[-seq_length:]\n",
    "        test_text_X = np.array([word2idx[c] if c in word2idx else word2idx['UNK'] for c in test_text_X])\n",
    "        test_text_X = pad_sequences([test_text_X], maxlen=seq_length, padding='pre', value=word2idx['UNK'])\n",
    "\n",
    "        # output_idx = model.predict_classes(test_text_X)\n",
    "        output_idx_back = model.predict(test_text_X, verbose=0)\n",
    "        output_idx = output_idx_back.argmax(axis=-1)\n",
    "        \n",
    "        test_sentence += ' ' + idx2word[output_idx[0]]\n",
    "    \n",
    "    print()\n",
    "    print(test_sentence)\n",
    "    print()\n",
    "\n",
    "testmodelcb = tf.keras.callbacks.LambdaCallback(on_epoch_end=testmodel)\n",
    "\n",
    "history = model.fit(train_dataset.repeat(), epochs=50, steps_per_epoch=steps_per_epoch, callbacks=[testmodelcb], verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b858d972",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
